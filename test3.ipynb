{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "import datetime\n",
    "import random\n",
    "rs = 420\n",
    "random.seed(rs)\n",
    "\n",
    "print(datetime.datetime.today().strftime(\"%HH:%MM %dd\"))\n",
    "# Load datasets\n",
    "\n",
    "\n",
    "vessel_encoder = LabelEncoder()\n",
    "\n",
    "# Select features and target\n",
    "features = ['vesselId', 'hour', 'day', 'month', 'year', 'minute', 'yearBuilt', 'length', 'vesselType', 'breadth', 'CEU', 'DWT', 'GT', 'portLatitude', 'portLongitude', 'sch_hour', 'sch_minute']#['cog', 'sog', 'rot', 'heading', 'navstat', 'latitude', 'longitude', 'vesselId', 'portId', 'hour', 'day', 'month', 'year', 'minute']\n",
    "target = ['latitude', 'longitude']  # Predicting next position (latitude, longitude)\n",
    "\n",
    "def feature_engineering(data):\n",
    "\n",
    "    data['vesselType'].fillna(83, inplace=True) #data['vesselType'].mode() -> dette er 83 men funker ikke??\n",
    "    data['yearBuilt'].fillna(data['yearBuilt'].median(), inplace=True)\n",
    "    data['length'].fillna(data['length'].median(), inplace=True)\n",
    "    data['breadth'].fillna(data['breadth'].median(), inplace=True)\n",
    "    data['CEU'].fillna(data['CEU'].median(), inplace=True)\n",
    "    data['DWT'].fillna(data['DWT'].median(), inplace=True)\n",
    "    data['GT'].fillna(data['GT'].median(), inplace=True)\n",
    "    data['portLatitude'].fillna(0, inplace=True)\n",
    "    data['portLongitude'].fillna(0, inplace=True)\n",
    "    data['sch_hour'].fillna(0, inplace=True)\n",
    "    data['sch_minute'].fillna(0, inplace=True)\n",
    "    return data\n",
    "\n",
    "# Handle missing values (if any)\n",
    "#ais_train = ais_train.dropna(subset=features + target) #ais_train.replace(to_replace='None', value=np.nan).dropna()\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "X_0 = pd.read_csv('x_data_0.csv', sep=',')\n",
    "print(\"p1:\", X_0.tail())\n",
    "X_1 = pd.read_csv('x_data_1.csv', sep=',')\n",
    "print(\"p2:\", X_1.tail())\n",
    "X_2 = pd.read_csv('x_data_2.csv', sep=',')\n",
    "print(\"p3:\", X_2.tail())\n",
    "X_3 = pd.read_csv('x_data_3.csv', sep=',')\n",
    "X_4 = pd.read_csv('x_data_4.csv', sep=',')\n",
    "X_5 = pd.read_csv('x_data_5.csv', sep=',')\n",
    "X_6 = pd.read_csv('x_data_6.csv', sep=',')\n",
    "X_7 = pd.read_csv('x_data_7.csv', sep=',')\n",
    "X_8 = pd.read_csv('x_data_8.csv', sep=',')\n",
    "X_9 = pd.read_csv('x_data_9.csv', sep=',')\n",
    "X_10 = pd.read_csv('x_data_10.csv', sep=',')\n",
    "X_11 = pd.read_csv('x_data_11.csv', sep=',')\n",
    "X_12 = pd.read_csv('x_data_12.csv', sep=',')\n",
    "X_13 = pd.read_csv('x_data_13.csv', sep=',')\n",
    "X_14 = pd.read_csv('x_data_14.csv', sep=',')\n",
    "X_15 = pd.read_csv('x_data_15.csv', sep=',')\n",
    "X_16 = pd.read_csv('x_data_16.csv', sep=',')\n",
    "X = X_0.join([X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9, X_10, X_11, X_12, X_13, X_14, X_15, X_16])\n",
    "y_1 = pd.read_csv('y_data_1.csv', sep=',')\n",
    "print(\"p4:\", y_1.tail())\n",
    "y_2 = pd.read_csv('y_data_2.csv', sep=',')\n",
    "print(\"p5:\", y_2.tail())\n",
    "y = y_1.join(y_2)\n",
    "\n",
    "print(\"p6:\", X.tail())\n",
    "print(\"p7:\", y.tail())\n",
    "X['vesselId'] = vessel_encoder.fit_transform(X['vesselId'])\n",
    "X = feature_engineering(X)\n",
    "model =KNeighborsRegressor(n_neighbors=50)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=rs)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Train a RandomForest model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred = model.predict(X_val_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "print(f'Mean Absolute Error: {mse}')\n",
    "print(f'Root Mean Squared Error: {np.sqrt(mse)}')\n",
    "print(f'R2-score: {r2_score(y_val, y_pred)}')\n",
    "print(f'Variance Score: {explained_variance_score(y_val, y_pred)}')\n",
    "\n",
    "# Use the same features as training\n",
    "X_0 = pd.read_csv('x_test.csv', sep=',')\n",
    "X_1 = pd.read_csv('x_test_1.csv', sep=',')\n",
    "X_2 = pd.read_csv('x_test_2.csv', sep=',')\n",
    "X_3 = pd.read_csv('x_test_3.csv', sep=',')\n",
    "X_test = X_0.join([X_1, X_2, X_3])\n",
    "X_test['vesselId'] = vessel_encoder.transform(X_test['vesselId'])\n",
    "# Scale the test set\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Predict on the test set\n",
    "test_predictions = model.predict(X_test_scaled)\n",
    "\n",
    "# Save the predictions\n",
    "predictions_df = pd.DataFrame(test_predictions, columns=['longitude_predicted', 'latitude_predicted'])\n",
    "predictions_df.to_csv('ais_test_predictions.csv', index=True)\n",
    "print(datetime.datetime.today().strftime(\"%HH:%MM %dd\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
