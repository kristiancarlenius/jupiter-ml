{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\n",
    "import datetime\n",
    "from io import StringIO\n",
    "import random\n",
    "#!{sys.executable} -m pip install xgboost\n",
    "rs = 420\n",
    "random.seed(rs)\n",
    "\n",
    "print(datetime.datetime.today().strftime(\"%HH:%MM %dd\"))\n",
    "# Load datasets\n",
    "\n",
    "featuers = ['vesselId', 'hour', 'day', 'month', 'minute', 'vesselType', 'yearBuilt', 'length', 'breadth', 'CEU', 'DWT', 'GT', 'portLatitude', 'portLongitude', 'hour_sch', 'day_sch', 'month_sch', 'minute_sch', 'lat_shift_1', 'lon_shift_1', 'minute_shift_1', 'hour_shift_1', 'day_shift_1', 'month_shift_1', 'cog_1', 'heading_1', 'lat_shift_2', 'lon_shift_2', 'minute_shift_2', 'hour_shift_2', 'day_shift_2', 'month_shift_2', 'cog_2', 'heading_2', 'lat_shift_3', 'lon_shift_3', 'minute_shift_3', 'hour_shift_3', 'day_shift_3', 'month_shift_3', 'cog_3', 'heading_3', 'lat_shift_4', 'lon_shift_4', 'minute_shift_4', 'hour_shift_4', 'day_shift_4', 'month_shift_4', 'cog_4', 'heading_4', 'lat_shift_5', 'lon_shift_5', 'minute_shift_5', 'hour_shift_5', 'day_shift_5', 'month_shift_5', 'cog_5', 'heading_5', 'cog', 'heading']\n",
    "featuers2 = ['longitude', 'latitude', 'vesselId', 'hour', 'day', 'month', 'minute', 'vesselType', 'yearBuilt', 'length', 'breadth', 'CEU', 'DWT', 'GT', 'portLatitude', 'portLongitude', 'hour_sch', 'day_sch', 'month_sch', 'minute_sch', 'lat_shift_1', 'lon_shift_1', 'minute_shift_1', 'hour_shift_1', 'day_shift_1', 'lat_shift_2', 'lon_shift_2', 'minute_shift_2', 'hour_shift_2', 'day_shift_2', 'lat_shift_3', 'lon_shift_3', 'minute_shift_3', 'hour_shift_3', 'day_shift_3', 'lat_shift_4', 'lon_shift_4', 'minute_shift_4', 'hour_shift_4', 'day_shift_4', 'lat_shift_5', 'lon_shift_5', 'minute_shift_5', 'hour_shift_5', 'day_shift_5']\n",
    "target2 = ['cog', 'heading']\n",
    "vessel_encoder = LabelEncoder()\n",
    "timesteps = 6\n",
    "# test-test split\n",
    "liste = []\n",
    "for i in range(len(featuers)):\n",
    "    liste.append(pd.read_csv('x_data_'+str(i)+'.csv', sep=','))\n",
    "\n",
    "featuers = ['vesselId', 'hour', 'day', 'month', 'minute', 'vesselType', 'yearBuilt', 'length', 'breadth', 'CEU', 'DWT', 'GT', 'portLatitude', 'portLongitude', 'hour_sch', 'day_sch', 'month_sch', 'minute_sch', 'lat_shift_1', 'lon_shift_1', 'minute_shift_1', 'hour_shift_1', 'day_shift_1', 'lat_shift_2', 'lon_shift_2', 'minute_shift_2', 'hour_shift_2', 'day_shift_2', 'lat_shift_3', 'lon_shift_3', 'minute_shift_3', 'hour_shift_3', 'day_shift_3', 'lat_shift_4', 'lon_shift_4', 'minute_shift_4', 'hour_shift_4', 'day_shift_4', 'lat_shift_5', 'lon_shift_5', 'minute_shift_5', 'hour_shift_5', 'day_shift_5', 'cog', 'heading']\n",
    "X = liste[0].join(liste[1:])\n",
    "X = X[featuers]\n",
    "y_1 = pd.read_csv('y_data_1.csv', sep=',')\n",
    "y_2 = pd.read_csv('y_data_2.csv', sep=',')\n",
    "y = y_2.join(y_1)\n",
    "\n",
    "z = y.join(X)\n",
    "x2 = z[featuers2]\n",
    "y2 = z[target2]\n",
    "\n",
    "\n",
    "print(\"p6:\", X.tail())\n",
    "print(\"p7:\", y.tail())\n",
    "#TODO: create a second model, this one predicts the cog and heading, then after the prediction on the ais_test with both models, use it to update the shifted values to create a new prediction using the original model\n",
    "\n",
    "X['vesselId'] = vessel_encoder.fit_transform(X['vesselId'])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=rs)\n",
    "\n",
    "X_train2, X_val2, y_train2, y_val2 = train_test_split(x2, y2, test_size=0.2, random_state=rs)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "scaler2 = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "X_train_scaled2 = scaler2.fit_transform(X_train2)\n",
    "X_val_scaled2 = scaler2.transform(X_val2)\n",
    "\n",
    "\n",
    "model = ExtraTreesRegressor(n_estimators=180, random_state=rs)\n",
    "model2 = RandomForestRegressor(n_estimators=1, random_state=rs)\n",
    "# Train a RandomForest model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "model2.fit(X_train_scaled2, y_train2)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred = model.predict(X_val_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "print(f'Mean Absolute Error: {mse}')\n",
    "print(f'Root Mean Squared Error: {np.sqrt(mse)}')\n",
    "print(f'R2-score: {r2_score(y_val, y_pred)}')\n",
    "print(f'Variance Score: {explained_variance_score(y_val, y_pred)}')\n",
    "\n",
    "# Use the same features as training\n",
    "X_0 = pd.read_csv('x_test.csv', sep=',')\n",
    "X_1 = pd.read_csv('x_test_1.csv', sep=',')\n",
    "X_2 = pd.read_csv('x_test_2.csv', sep=',')\n",
    "X_3 = pd.read_csv('x_test_3.csv', sep=',')\n",
    "X_4 = pd.read_csv('x_test_4.csv', sep=',')\n",
    "X_test = X_0.join([X_1, X_2, X_3, X_4])\n",
    "X_test = X_test[featuers]\n",
    "X_test['vesselId'] = vessel_encoder.transform(X_test['vesselId'])\n",
    "# Scale the test set\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Predict on the test set\n",
    "test_predictions = model.predict(X_test_scaled)\n",
    "\n",
    "# Save the predictions\n",
    "predictions_df = pd.DataFrame(test_predictions, columns=['longitude', 'latitude'])\n",
    "last = X_test.join(predictions_df)\n",
    "X_test2 = X_test.join(predictions_df)\n",
    "\n",
    "for i in range(1, timesteps):  # Only 3 previous time steps\n",
    "    X_test2[f'lat_shift_{i}'] = X_test2.groupby('vesselId')['latitude'].shift(i)\n",
    "    X_test2[f'lon_shift_{i}'] = X_test2.groupby('vesselId')['longitude'].shift(i)\n",
    "    X_test2[f'minute_shift_{i}'] = X_test2.groupby('vesselId')['minute'].shift(i)\n",
    "    X_test2[f'hour_shift_{i}'] = X_test2.groupby('vesselId')['hour'].shift(i)\n",
    "    X_test2[f'day_shift_{i}'] = X_test2.groupby('vesselId')['day'].shift(i)\n",
    "\n",
    "    X_test2[f'lat_shift_{i}'] = np.where(X_test2[f'lat_shift_{i}'].isna(), last[f'lat_shift_{i}'], X_test2[f'lat_shift_{i}'])\n",
    "    X_test2[f'lon_shift_{i}'] = np.where(X_test2[f'lon_shift_{i}'].isna(), last[f'lon_shift_{i}'], X_test2[f'lon_shift_{i}'])\n",
    "    X_test2[f'minute_shift_{i}'] = np.where(X_test2[f'minute_shift_{i}'].isna(), last[f'minute_shift_{i}'], X_test2[f'minute_shift_{i}'])\n",
    "    X_test2[f'hour_shift_{i}'] = np.where(X_test2[f'hour_shift_{i}'].isna(), last[f'hour_shift_{i}'], X_test2[f'hour_shift_{i}'])\n",
    "    X_test2[f'day_shift_{i}'] = np.where(X_test2[f'day_shift_{i}'].isna(), last[f'day_shift_{i}'], X_test2[f'day_shift_{i}'])\n",
    "\n",
    "last = X_test2[featuers]\n",
    "X_test2 = X_test2[featuers2]\n",
    "X_test_scaled2 = scaler2.transform(X_test2)\n",
    "#TODO: use predictions_df in the slides for the next two predicts\n",
    "\n",
    "\n",
    "test_predictions2 = model2.predict(X_test_scaled2)\n",
    "headercogup = pd.DataFrame(test_predictions2, columns=target2)\n",
    "\n",
    "last['cog'] = headercogup['cog']\n",
    "last['heading'] = headercogup['heading']\n",
    "\n",
    "last_scaled = scaler.transform(last)\n",
    "\n",
    "final_prediction = model.predict(last_scaled)\n",
    "\n",
    "predictions_df = pd.DataFrame(final_prediction, columns=['longitude_predicted', 'latitude_predicted'])\n",
    "predictions_df.to_csv('ais_test_predictions_1.csv', index=True)\n",
    "print(datetime.datetime.today().strftime(\"%HH:%MM %dd\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
